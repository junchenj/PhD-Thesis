\chapter{Lessons, Limitations, and Future Work}
\label{ch:concl}

%Ensuring high QoE is crucial for today's application providers,
%whose revenues today are advertisement-/subscription-driven.
%In this dissertation, we argue that classic networking approaches have
%failed to achieve QoE needed by these applications, and that
%a new paradigm called data-driven networking (\ddn) has the 
%potential to improve QoE by harnessing the massive real-time 
%measurement data passively collected from millions of end users.

In this chapter, we conclude the dissertation by 
(1) summarizing its key contributions (Section~\ref{sec:concl:contributions}), 
(2) highlighting lessons learned from our implementation and 
deployment of proposed solutions (Section~\ref{sec:concl:lessions}), 
(3) discussing fundamental limitations of our solutions (Section~\ref{sec:concl:limitations}), and
(4) identifying several key research topics and outlining a 
research agenda to help the networking community recognize 
and embrace the data-driven networking paradigm (Section~\ref{sec:concl:future}), .

\section{Summary of Contributions}
\label{sec:concl:contributions}
%\jc{
%\begin{itemize}
%\item two basic approaches
%\item The key contribution is to enable \ddn
%\item Three systems: cfa, pytheas, via
%\item real world deployment
%\end{itemize}
%}

Over the past decades, intense efforts have been made towards
ensuring high quality of Internet applications by following two 
traditional approach: the {\em in-network} approach and 
the {\em endpoint} approach.
Ideally, to optimize QoE by adapting to the dynamic network conditions,
we need to accurately measure both {\em real-time network 
conditions} and {\em user-perceived QoE}, but both approaches fail
on at least one front, and are thus ill-suited to meet the 
requirements of today's applications. 
On one hand, the in-network approach does not have accurate 
information on user-perceived QoE and requires costly 
re-architecting ISPs' infrastructure.
On the other hand, the endpoint approach has to infer network
conditions from limited and often noisy information available locally
to individual end users.
As a result, we have seen that a substantial fraction of video 
viewers and VoIP users suffer from suboptimal quality.

The main contribution of this dissertation is to demonstrate the
feasibility of {\em improving QoE by accurately predicting network conditions
based on a real-time global view of QoE measured on millions of 
end users, thus achieving 
the best of both worlds of classic in-network and endpoint approaches.}
We call this new paradigm Data-Driven Networking (\ddn).
To make \ddn practical, this dissertation designs 
and implements algorithms and end-to-end systems of \ddn to
achieve non-trivial QoE improvement for video streaming and 
Internet telephony in real-world deployment.

%In particular, we have developed complete solutions to address
%the two challenges that are key to unleashing the full potential of \ddn.
The key insight underlying this dissertation is that there are some 
domain-specific {\em persistent structures} in the complex relation 
among session-level features, decisions, and QoE. 
This insight has inspired our solutions to address the two 
challenges that are key to unleashing the full potential of \ddn.

\mypara{Expressive models}
The main algorithmic challenge of \ddn is that we need a model 
expressive enough to capture the high-dimensional relations 
between session-level features, decisions, and QoE. 
The insight of persistent structures have helped address
this challenge on two fronts.
\begin{itemize}
\item First, these structures can be used to reduce the complexity of 
the problem by identifying the most 
important features and the most promising decisions.
\item Second, learning these structures might be hard, 
since it often needs more data than making a real-time decision 
based on known structures. But the fact that these structures tend
to  persistent on longer timescales than QoE itself
suggests that we can use a longer time window
and learn the structures from more history data.
\end{itemize}
For instance, in CFA (Chapter~\ref{ch:cfa}) and DDA
(Chapter~\ref{ch:dda}), we learn a small 
subset of features which a session's QoE depends on,
and in VIA (Chapter~\ref{ch:via}), Guided Exploration reduces
the large decision spaces in Internet telephony by focusing
on only a subset of the most promising relay paths.



\mypara{Scalable platforms}
The system challenge of \ddn is how to make real-time decisions
for millions of geo-distributed clients based on a fresh, global
view of client-side QoE.
The insight of persistent structures have helped address
this challenge on two fronts.
\begin{itemize}
\item First,  these structures often correlate with network locality,
so they enable an effective spatial decomposition of the global 
data-driven process
into smaller-scale subprocesses which are naturally amenable for 
a scale-out implementation in today's cloud infrastructure.
\item Second, because these structures are persistent, their 
learning process, which requires a global view, can be decoupled
from real-time decision-making process and run on a separate
timescale.
\end{itemize}
For instance, in Pytheas (Chapter~\ref{ch:pytheas}), Group-based 
Exploration-Exploitation (E2) decomposes the global E2 process
into independent E2 processes each controlling a group of 
clients sharing network locality (and other key features)
and running in a geo-distributed frontend
cluster with fresh data of these clients.
As a result, we have shown that Pytheas can scale horizontally 
and can make decision for a population of a site like YouTube (5
billion users per day) with measurement data of 
concurrent sessions in less than a second of delay.


\mypara{QoE improvement} 
Using a combination of real-world deployment, offline emulation
driven by large-scale measurement of real traffic, we have shown
that the solutions proposed in this dissertation can lead to substantial 
QoE improvement in video streaming and VoIP. 
For instance, in video streaming, we demonstrate that CFA leads to  
32\% less buffering time than a baseline random decision maker,
and Pytheas leads to a further 31\% reduction on buffering time
over CFA.
In Internet telephony, we use trace-driven analysis
and a small-scale deployment shows that VIA cuts the incidence
of poor network conditions for calls by 45\% (and for
some countries and ASes by over 80\%) while staying within
a budget for relaying traffic through the managed overlay.



\section{Lessons Learned}
\label{sec:concl:lessions}
%\jc{
%\begin{itemize}
%\item Importance of offline simulation
%\item Cost of centralized control
%\item Need for the explanatory power
%\end{itemize}
%}

%In this section, we discuss some lessons learned from
%the collaboration with industry and the deployment of \ddn.

\mypara{Offline evaluation}
One of the practical challenges we encountered in working with
Conviva and Microsoft Skype has been how to demonstrate the 
\ddn's benefit with sufficient confidence in an offline fashion, 
i.e., no modification to the production system.
A natural solution is to use simulation driven by measurement
trace collected from real traffic, but this could be greatly 
biased by how
the trace were collected; e.g., we would not able to evaluate
the performance of Akamai in Pittsburgh if Pittsburgh 
users have only used other CDNs. 
While a full discussion on how to build an accurate trace-driven 
simulator (e.g.,~\cite{tariq2008answering}) is beyond our scope, 
we found that it would be very useful to randomize the decisions 
even on a small portion of sessions 
when collecting trace for offline evaluation, as it 
allows us to estimate the outcome of alternative decisions.

\mypara{Gradual deployment}
Another consideration equally critical to these application 
providers is the ability to gradually roll-out the proposed 
solutions in real production settings to gain sufficient confidence
before deploying them to more traffic. This could 
take various forms--
besides A/B testing, we also found sometimes even a 
partial deployment of the proposed solution could be useful as well.
For instance, while it is practically difficult to deploy 
VIA on today's Skype relaying system which does not 
directly support changing relays on a per-call basis
as needed by VIA, we have found it beneficial to 
even deploy the offline relay pruning part, because it only needs to
be updated on a coarse timescale, and still offers rough prediction
on which relays are more promising. 
From our experience, we believe that it is sometimes 
necessary to trade some performance benefit for an implementation
compatible to the existing systems for practical reasons.




\mypara{Leveraging application-specific resilience}
A key enabler for the scale-out and fault-tolerant architecture of 
Pytheas and parallel industry efforts such as C3~\cite{c3} 
is that we
were able to  exploit domain-specific
properties that allows us to weaken some requirements.
For instance, Pytheas tolerates controller failures by leveraging
the fact that without the Pytheas controller, video players can 
still stream videos and quickly fall back to use the built-in local
adaptation logic react to
network conditions, 
albeit with suboptimal quality. In essence, our insight of 
persistent structures can be also viewed as an instantiation of
application-level resilience.
Though it is always more desirable to have a general-purpose 
solution, we believe it might be
possible to leverage application-specific resilience and
engineer simpler schemes to meet the requirements of 
specific applications.


%While our implementation of CFA and Pytheas, together
% with parallel industry efforts such as C3~\cite{c3}, 
% have demonstrated a 
%feasible path towards centralizing key decisions of 
%client-side adaptation, we observe two clear risk 
%across these projects to centralize all decisions traditionally
%made by client-side.


%\mypara{Cost of centralized control}

\mypara{Need for interpretability}
Our conversations with domain experts revealed that they 
were apprehensive about using complex machine learning 
models (e.g., SVM or PCA) that use non-intuitive ``projections'' 
of features. The reason is that the decisions made by these 
algorithms are not mappable 
to real-world effects (e.g., which CDN) that are 
critical for diagnostics and incident response.
Therefore, we also strove to integrate our solutions with
interpretability~\cite{vellido2012making}, 
allowing domain experts to combine
their knowledge and diagnose prediction
errors or resolve incidents.
For instance, a practical benefit of CFA is that when we observe
high prediction errors, we can check the critical features
based on which predictions were made and try to correlate them 
with known network incidents.


\section{Limitations}
\label{sec:concl:limitations}

\mypara{Root cause diagnosis}
While our algorithms provide the explanatory power of how a 
prediction or a decision is made, they are not design for 
root cause diagnosis.
For instance, in CFA, critical features are not a minimal set of
factors that determine the quality (i.e., root cause).
That is, they can include both features that reflect
the root cause as well as additional features.
For example, if all HBO sessions use Level3, their
critical features may include both \fCDN and \fSite,
even if \fCDN is redundant, since including it does
not alter predictions.
The primary objective of \dda is accurate prediction;
root cause diagnosis may be an added benefit.
In essence, this dissertation aims at building a 
statistics correlation between features, decisions,
and QoE to help improve QoE, but causal analysis is 
fundamentally different and is known to be generally
more difficult than identifying statistic correlations.

\mypara{Skewed visibility}
While driving decision making by QoE observed by
millions of application sessions enjoys many advantages, 
the view on network conditions provided by these sessions
is fundamentally skewed in two aspects.
(1) On one hand, since many applications 
such as video streaming rely
heavily on CDNs to push content closer to clients, 
network paths of today's application sessions are skewed 
towards the paths from edge servers/caches to clients.
(2) On the other hand, application quality is generally 
imbalanced with most sessions having good quality, but it is 
the identification of bad quality that leads to more 
QoE improvement by avoiding bad decisions.
For instance, in Section~\ref{sec:measurement:video}, 
\problemclusters 
usually do not have over 50\% bad-quality sessions, 
but the fraction is still significantly higher than the 
global average.


\mypara{Handling flash crowds}
Flash crowds happen when many sessions join at the same 
time and cause part of the resources (decisions) to be overloaded.
While Pytheas can handle load-induced QoE fluctuations that occur 
in individual groups, overloads caused by flash crowds are different, in 
that they could affect sessions in multiple groups. Therefore, those 
affected sessions need to be regrouped immediately, but Pytheas 
does not support such real-time group learning. 
To handle flash crowds, Pytheas needs a mechanism to detect flash 
crowds and create groups for the affected sessions in real time.

%\mypara{Use of active measurement}
%While our system relies entirely on passive QoE measurements as input of data-driven optimization, we see a new opportunity to augment the system with active measurements by orchestrating vantage points and controlled clients (e.g., Skype~\cite{via}) to establish fake traffic.
%These active measurement can be used to explore suboptimal decisions, thereby reducing the cost of \mab on real sessions. 
%One also needs to take into consideration the additional load due to these active measurements.


\mypara{Agnostic to cost of switching decisions}
For now, our solutions assumes there is little cost to switch the decision
during the course of an application session or to
switch a large portion of clients from one decision to another. 
While such assumption applies to today's DASH-based video 
streaming protocols~\cite{dash-standard}, other applications (e.g., VoIP) 
may have significant cost when switching decisions in the middle of a 
session, so the control logic should not too sensitive to QoE fluctuations.
Moreover, a content provider pays CDNs by 95th percentile traffic, 
so we must carefully take the traffic distribution into account as well.
We intend to explore decision-making logic that is 
aware of these costs of switching decisions in the future.


\mypara{Limited decision space}
A final limitation of \ddn approach is that its potential room of
improvement is fundamentally limited by the granularity of the 
``control knobs'' exposed
by  the underlying delivery system. 
For instance, in video streaming, our solutions currently select resources
at the CDN granularity. This means we cannot
do much if the CDN redirects the client based on its location
and the servers the CDN redirects the client to are
congested. However, if the client were able to specify
the server to stream from, we could avoid the overloaded
servers and improve quality.


%\jc{
%\begin{itemize}
%\item Root cause diagnosis
%\item Skewed measurement -> active measurement
%\item No cost of switching decisions
%\item Limited decision space
%\end{itemize}
%}

\section{Future Work}
\label{sec:concl:future}

Inspired by this dissertation, we believe there 
will be tremendous opportunities for 
data-driven techniques in networked and distributed systems 
driven by both ``use pulls'' (high QoE/performance requirements and 
the increasing complexity of networked systems) and ``technology 
pushes'' (availability of big data in networking and new data 
analytics capability).
This section identifies several future research directions
to explore this confluence of data-driven paradigms and 
networked and distributed systems.

\subsection{Rethinking Classic Problems in Networking}

The data-driven paradigm inspires rethinking many classic 
problems in networking as well as enabling new services. 

\mypara{End-to-end adaptation}
As the Internet grows more complex, it has become increasingly difficult for transport protocols (e.g., TCP) and applications (e.g., Facebook Live) to adapt in real time to changes in network conditions and resource availability.
I believe data-driven techniques offer a promising new approach. Consider TCP;
I envision a new service that aggregates real-time performance of similar TCP connections and {\em predicts} the largest window size for which a TCP connection will not experience congestion losses.
This service can potentially lead to better TCP performance than many state-of-the-art techniques, as it needs little active probing and can train the logic in near real time, rather than offline. 
An early promise of this approach was shown in my prior work of CS2P~\cite{cs2p} which can accurately predict HTTP throughput using information of similar HTTP sessions, and CS2P could potentially be extended to inferring optimal window sizes from the HTTP throughput prediction.

\mypara{Internet traffic map}
Many Internet services can benefit from a traffic map service that can predict performance (e.g., available bandwidth) of any network path, but prior efforts towards such a service suffer from limited coverage, high probing overhead, or significant data staleness. 
%what's new
Fortunately, passive measurements of applications like video streaming offer a new enabler for such a service, as they provide real-time measurements of network state from millions of vantage points without incurring any probing overhead!
My prior work~\cite{via,sun2014using} shows it is possible to predict coarse-grained performance metrics (e.g., RTT between ISPs) by mapping video quality to simple network models.
Extending these maps to predicting fine-grained (e.g., link-level) metrics provide an interesting next step.

\mypara{Systematic trace-driven evaluation}
The first question for any application provider (e.g., Netflix) is
that before implementing DDN, ``can I quantify how much DDN would actually improve
my application's quality (e.g., video quality)?''
This is also known in ML literature as {\em off-policy evaluation}.
It is, however, not straightforward to perform off-policy evaluation
based on passively collected network traces, because data could be collected
through a biased process or have high variance.
A promising alternative is the recently proposed
{\em doubly robust} (DR) estimator~\cite{doublerobust}.
While the DR is a promising starting point,
there are network-specific factors it does not consider. For instance, the DR
estimator will not identify quality degradation due to server overload, if such overload
never happens in the dataset.

\subsection{Towards a Common Architecture for Data-Driven Networking}

As a diverse set of applications can benefit from \ddn, we envision a common software stack and network architecture support to  allow
us to extract a general set of reusable design principles and 
solutions to avoid the problem of each
effort reinventing the wheel or worse ignoring some potential pitfalls.

\mypara{Custom Data Analytics Stack for Networking}
While data-driven techniques can be applied to many problems, there are several {\em common} challenges across these applications.
%\begin{packeditemize}
%\item {\em Network-specific knowledge:} 
First, while it is tempting to use general-purpose techniques to analyze networking data, one must take into account {\em network-specific knowledge} to avoid undesirable outcomes; 
e.g., to optimize overall performance, ESPN may use small ISPs to use suboptimal CDNs and let Comcast users use optimal CDNs, causing a reverse network neutrality violation in which content providers discriminate against ISPs!
%\item {\em Application-specific resilience:}
Second, one also needs to leverage {\em application-specific resilience}; e.g., to probe both optimal and suboptimal servers without affecting quality, a video player can use the optimal server to fetch most content, but use suboptimal servers when the buffer is full. 
%\end{packeditemize}
While data-driven approaches will have more benefits if these network-/application-specific opportunities can be utilized in a systematic way, unfortunately, there are few formal approaches to integrating them with existing ``black-box'' algorithms.
To best utilize various network-/application-specific opportunities, I envision a {\em custom data analytics stack for networking} built on top of existing analytics stacks (e.g., Spark), which offers abstractions to express network/application concepts (e.g., sessions) and requirements (e.g., data staleness).
It is helpful to draw a parallel to the efforts of building an analytics stack for graph processing~\cite{graphx}, where researchers realized the limitations of Spark in supporting graph processing operations, and customized Spark and built a software layer over it to support APIs specific for graph processing.

\mypara{Architecture of Cross-Provider Data Sharing}
So far my work has been primarily within the scope of a single service provider, looking forward I argue that there is greater room for improving application QoE by bringing {\em more} service providers into the loop. These providers can be ISPs, CDNs, and cloud services, whose revenue is also driven, albeit indirectly, by high QoE.
Unfortunately, in today's federated Internet structure, many problems arise because each service provider has little visibility into either the QoE of its sessions or the decisions made by others.
%This causes many obstacles to QoE optimization, 
%e.g., Amazon AWS cannot diagnose whether a spike in service latency of Comcast users is caused by itself or by Comcast, as well as how much the latency spike affects user experience.
For instance, ESPN clients use HTTP-based bitrate-adaptation video players, which can select both CDN (e.g., Akamai and Limelight) and bitrate, and in many cases these players perform poorly because ESPN cannot know whether the bottleneck link is at edge ISPs (e.g., Comcast) or CDN servers (e.g., Akamai).
Now, if Comcast shares with ESPN additional information that attributes bottlenecks to Comcast rather than Akamai, the players can react to ISP congestion by lowering the bitrate, rather than blindly switching between Akamai and Limelight. 
Building on this intuition and my early work~\cite{eona}, I argue that the fundamental problem lies in the limited interfaces between service providers, and that a principled approach to re-architecting these interfaces with explicit QoE optimization in mind is needed.
I envision an {\em Experience-Oriented Network Architecture} ({\em EONA}), where service providers share QoE information and key configuration changes in order to optimize user-perceived QoE in a coordinated way, while still keeping the federated structure.
EONA can be realized without changing the data/control plane protocols; instead, it adds new interfaces between the ``controllers'' of service providers (e.g., an SDN controller, a cloud orchestrator, or a global video control plane~\cite{sigcomm12}).
Despite EONA's potential, there are many open-ended questions: 
How to incentivize EONA to attract service providers?
What is the minimal set of information that has to be shared? 
How to preserve privacy when sharing data between providers?

\subsection{Other Directions}

\mypara{Data-Driven Configurations in Internet of Things}
%% pervasive analytics across low-capacity devices is one key feature of IoT
The past few years have witnessed a dramatic increase in 
Internet of Things (IoT) devices and the proliferation of their applications. One 
distinctive feature of
these IoT applications is that they emphasize more on data analytics than 
traditional network applications. For instance, unlike traditional video
streaming, most IoT-generated videos are not streamed for viewers 
to watch, but used  to extract useful 
information from the video feed. 
Moreover, these analytics tasks have extremely heterogeneous and dynamic 
characteristics.
They have diverse requirements (e.g., delay sensitive vs. accuracy sensitive),  
capacity of the devices (e.g., general-purpose machines vs. 
specialized hardware), and power duration.
Moreover, as these devices need to constantly interact with users, 
their workloads are more dynamic, if not more unpredictable, than traditional
Internet-connected devices.
Fortunately, we observe that recent advances in ML and system research 
have offered to 
many ``knobs'' to cope with such a heterogeneity, including 
tuning hype-parameters of deep neural networks, 
flexible consistency guarantees in database, and various types of
cloud offerings).
%% we observe an opportunity here by dynamically optimizing the system
The challenge however is how to dynamically identify the configurations
that are suitable to each analytics task at any point of time.
%% rule-based approach is not working
We argue that it would be suboptimal and unsustainable to rely on handcrafted
static rules to meet the requirement of any application and 
operating environments.
A parallel can be drawn from many similar problems, such as parameter
selection in congestion control and cloud configuration selection, where
static rule-based logic has failed to handle dynamic workload and
requirements of the underlying task.
%% data-driven is the solution
Instead, we argue a more systematic solution is to leverage data-driven
techniques to learn the best configurations of these knobs based on
real-time workload and monitoring data.
A case in point is the selection of configurations in video 
analytics~\cite{zhang2017live}, which seeks to strike a dynamic 
balance between
timeliness and fidelity.
While conceptually similar to hyper-parameter training in 
ML, data-driven adaptation in IoT poses system challenges that have 
not been studied so far; including how to scale the process to millions of
devices who has limited bandwidth and capacity.


\mypara{Infonomics of Network Measurement Data}
%% a key challenge in using measurement data is xxx
A key challenge to democratize of the benefit 
data-driven paradigm in networking is that most measurement 
data are collected 
and maintained by application providers (e.g., video sites, 
web sites) or infrastructure providers (e.g., ISP, CDNs), so 
it would be impractical to assume they will freely share 
the insight extracted from their data for a common goal.
%% prior work has focused on privacy
To address the issue, prior work has largely focused on how to ensure data privacy
by techniques like differential privacy. 
These efforts, however, have overlooked many key issues, including the 
incentive structure of data sharing, and how data fidelity (timeliness, 
granularity, etc) affect the willingness of others to use these data.
%% pricing the key towards a common framework
We argue that a more effective and comprehensive solution is to
develop a marketplace of network measurement data, where the key
is a {\em pricing} framework to quantify the value of these 
network measurement data as well as the willingness of others to 
purchase them rather than learning it from what they already have.
In this context, we can think of learning of BGP path as a special case of 
such marketplace 
for sharing reachability information, where people have spent several decades
to reach a pricing model for each ISP charges or peers with 
neighboring ISPs.
%A special case of this framework adopt simple ``peering'' protocols to allow sharing
%of data, just as in exchange of reachability information in inter-domain 
%routing.
%% can borrow ideas from infonomics
Such marketplace of network measurement data can be viewed
as an instance of infonomics~\cite{infonomics}, an emerging notion in economics
to measure, manage and monetize information as a real asset.
%% challenge: how to formulate the net-specific concerns


\section{Final Remark}

The past decade has witnessed the coming of age of data-driven
paradigm in various aspects of computing (partly) empowered by advances
in networked and distributed systems (cloud computing, MapReduce, etc).
The overarching argument of this dissertation is that the benefits 
can flow the opposite direction as well:
{\em networked systems can be improved
by data-driven paradigm} -- 
by leveraging a dramatically increased amount of measurement data
and the state-of-the-art ML and large-scale data analytics techniques,
we can unleash the ``unreasonable effectiveness of data'' 
in networking.

%This dissertation is driven by two long-term technology trends. 
%On one hand, we observe several ``use pulls'', including that applications
%and services that use the Internet are increasingly demanding, 
%and that networked systems are becoming increasingly dynamic and 
%heterogeneous, necessitating a new approach to handle the growing complexity.
%On the other hand, we observe several ``technology pushes'', including 
%new tools to collect massive data from various network devices (e.g., 
%client-side instrumentation, SDN), and recent advances 
%of large-scale analytics platforms.






%In this dissertation, we demonstrate that 
%by leveraging a dramatically increased amount of measurement data
%and the state-of-the-art ML and large-scale data analytics techniques,
%we can unleash the ``unreasonable effectiveness of data'' 
%in networking.
%
%This dissertation is driven by two long-term technology trends. 
%On one hand, we observe several ``use pulls'', including that applications
%and services that use the Internet are increasingly demanding, 
%and that networked systems are becoming increasingly dynamic and 
%heterogeneous, necessitating a new approach to handle the growing complexity.
%On the other hand, we observe several ``technology pushes'', including 
%new tools to collect massive data from various network devices (e.g., 
%client-side instrumentation, SDN), and recent advances 
%of large-scale analytics platforms.
%
%there are two trends: on one hand, the networked systems are becoming increasingly complex, XXX
%And at the same time, we now have the means to collect massive data from various network devices, XXX
%Now, if we put these two trends together, we can clearly see that there are tremendous opportunities to apply ML and data science in system research.
%Looking through this lens, my phd thesis is only a beginning, and in the coming years, I believe there will be many innovations in this new area.

%which has transformed the landscape of many research fields 
%(such as natural language processing and computer vision) in the recent years.
%Inspired by these recent successes of data-driven approaches, 
%I believe networking should be no difference. 


%The last decade has witnessed the coming of age of data-driven
%paradigm in various aspects of computing (partly) empowered by advances
%in distributed and networked systems (cloud computing, MapReduce, etc).
%The overarching argument of this dissertation is that the benefits 
%can flow the opposite direction:
%the design and management of networked systems can be improved
%by data-driven paradigm.
%
%This dissertation demonstrates the promise that 
%by using more data, networking research can benefit from the 
%``unreasonable effectiveness of data'', 
%which has transformed the landscape of many research fields 
%(such as natural language processing and computer vision) in the recent years.
%%Inspired by these recent successes of data-driven approaches, 
%I believe networking should be no difference. 




